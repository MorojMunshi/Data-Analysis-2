{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorojMunshi/Data-Analysis-2/blob/main/Lab_1_Exploring_LSTM_and_GRU_Architectures_in_Sequence_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction:\n",
        "In this lab, you will explore the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures—two advanced recurrent neural network (RNN) models designed to handle sequential data effectively.\n",
        "\n",
        "The main objectives of today lab are as follow:\n",
        "\n",
        "1. Train and test LSTM and GRU models on three types of sequence modeling tasks:\n",
        "\n",
        "    * One-to-Many: Sequence generation from a single input.\n",
        "    * Many-to-One: Classifying a sequence based on its pattern.\n",
        "    * Many-to-Many: Translating one sequence into another.\n",
        "\n",
        "2. Test the performance of these models.\n",
        "3. Compare the results of LSTM and GRU for each task."
      ],
      "metadata": {
        "id": "uO4eXVZo86_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task one: One-to-Many\n",
        "\n",
        "First, Synthetic datasets are used to simplify implementation and focus on learning model mechanics.\n",
        "\n",
        "The task is to Generate a sequence of integers starting from a single input integer. So, our trining data is an inte"
      ],
      "metadata": {
        "id": "6Kky7RA__ErU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate dataset\n",
        "def generate_one_to_many_data(size=1000):\n",
        "    X = np.random.randint(1, 100, size=(size, 1))  # Single numbers\n",
        "    y = np.array([np.arange(x, x + 5) for x in X.flatten()])  # Sequence of 5 numbers\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_one_to_many_data()"
      ],
      "metadata": {
        "id": "E9y-dAdz_AxD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print the first 10 rows from X and y, so u undersand how it looks like."
      ],
      "metadata": {
        "id": "khC9CFquBIOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the # Print the first 10 rows of X and y\n",
        "\n",
        "print(\"First 10 inputs (X):\")\n",
        "\n",
        "print(X[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMlGi-BKBfXg",
        "outputId": "85ce9a28-db74-4c84-afc7-674e14317d23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 inputs (X):\n",
            "[[93]\n",
            " [69]\n",
            " [63]\n",
            " [86]\n",
            " [66]\n",
            " [61]\n",
            " [78]\n",
            " [29]\n",
            " [76]\n",
            " [19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqUdkyMPA1zA",
        "outputId": "a74d9855-9b10-4716-e2b0-97bc854b0f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[76, 77, 78, 79, 80],\n",
              "       [26, 27, 28, 29, 30],\n",
              "       [ 7,  8,  9, 10, 11],\n",
              "       [90, 91, 92, 93, 94],\n",
              "       [77, 78, 79, 80, 81],\n",
              "       [82, 83, 84, 85, 86],\n",
              "       [46, 47, 48, 49, 50],\n",
              "       [ 9, 10, 11, 12, 13],\n",
              "       [ 1,  2,  3,  4,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Implementation and Training:\n",
        "Implement and train two sequence modeling architectures: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). follow these steps:\n",
        "\n",
        "1. __Build the Models:__ Define LSTM and GRU models using the Keras library,specifying the number of neurons, input shapes, and layers. (Hint: Use the Sequential API in Keras to stack layers. Start with an LSTM or GRU layer, followed by a dense layer to generate the output sequence.)\n",
        "\n",
        "2. __Compile the Models:__ Choose appropriate loss functions and optimizers to train the models efficiently. (Hint: Since our data consists of numerical values, use mean squared error (MSE) as the loss function for regression tasks. Use an optimizer like Adam for faster convergence.)\n",
        "\n",
        "3. __Train the Models:__ Use the training dataset to adjust model weights through backpropagation, minimizing the loss over multiple epochs. Do not forget to add a validation set during training to monitor for overfitting and fine-tune the model.\n",
        "(Hint: Ensure the input data is correctly shaped to match the expected input dimensions of the LSTM or GRU (samples, timesteps, features). Use a batch size that balances speed and memory usage (e.g., 32).)"
      ],
      "metadata": {
        "id": "AygcwO8jBvP5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0R9MbzcvjCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gte55qZevjKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input for LSTM/GRU\n",
        "X = X.reshape(-1, 1, 1)  # (samples, timesteps, features)\n",
        "y = y.reshape(-1, 5)  # (samples, timesteps)"
      ],
      "metadata": {
        "id": "aRRW-UyOIxgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWEZKmn9B1VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the model\n",
        "\n",
        "Test both of your models on a single input of 10 and predict the output sequence. Verify if the output matches the expected sequence (10, 11, 12, 13, 14). Note that your model will produce floating-point numbers; to quickly convert them to integers, you can use the np.round function."
      ],
      "metadata": {
        "id": "KV_671baDUzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.array([10]).reshape(-1, 1, 1)  # Test input"
      ],
      "metadata": {
        "id": "f_lQPfGwDZGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjXDx8waGbkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Improve model performance\n",
        "If your model does not perform well on the test data, try one of the following strategies to improve its performance:\n",
        "\n",
        "* Increase Training Epochs: Train the model for more epochs to allow it to better learn the patterns in the data.\n",
        "* Increase Model Complexity: Add more neurons or additional layers to enhance the model's capacity to capture complex relationships.\n",
        "* Adjust the Learning Rate: If the loss is fluctuating or convergence is slow, use a learning rate scheduler or manually modify the optimizer's learning rate."
      ],
      "metadata": {
        "id": "I0ehl3jgLM42"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ud4g4ANENGLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task Two: Many-to-One Task – Sequence Classification\n",
        "The task is to Classify sequences as increasing (1) or decreasing (0).\n",
        "\n",
        "*Dataset* Preparation:"
      ],
      "metadata": {
        "id": "pS-ySj04FqP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Generate training dataset\n",
        "def generate_many_to_one_data(size, seq_length=5):\n",
        "    # Half of the dataset will have increasing sequences (label = 1)\n",
        "    # Half of the dataset will have decreasing sequences (label = 0)\n",
        "    X2 = np.zeros((size, seq_length))\n",
        "    y2 = np.zeros(size)\n",
        "\n",
        "    for i in range(size // 2):\n",
        "        # Generate increasing sequence (label = 1)\n",
        "        X2[i] = np.arange(i+1, i + seq_length + 1)\n",
        "        y2[i] = 1  # Increasing sequence\n",
        "\n",
        "        # Generate decreasing sequence (label = 0)\n",
        "        X2[size // 2 + i] = np.arange(i + seq_length, i, -1)\n",
        "        y2[size // 2 + i] = 0  # Decreasing sequence\n",
        "\n",
        "    return X2, y2\n",
        "\n",
        "# Generate a balanced dataset\n",
        "X2, y2 = generate_many_to_one_data(1000)\n",
        "\n",
        "# Shuffle the dataset\n",
        "X2, y2 = shuffle(X2, y2, random_state=42)\n",
        "\n",
        "# Reshape input for LSTM/GRU\n",
        "X2 = X2.reshape(-1, 5, 1)"
      ],
      "metadata": {
        "id": "AdqK23Lh74W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Implementation and Training:\n",
        "Implement and train two sequence modeling architectures: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). follow the same steps as you did in task 1\n"
      ],
      "metadata": {
        "id": "tZEUU2gDI2nd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GS781jjM5TA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the model"
      ],
      "metadata": {
        "id": "b85FICO-JIbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a balanced dataset\n",
        "X2_test, y2_test = generate_many_to_one_data(10)\n",
        "\n",
        "# Reshape input for LSTM/GRU\n",
        "X2_test = X2_test.reshape(-1, 5, 1)"
      ],
      "metadata": {
        "id": "JCRacAnkK3Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Improve model performance\n",
        "if your model does not produce the true labels, try to improve it using the techniques you applied in Task 1."
      ],
      "metadata": {
        "id": "ZoLYdyGlK30G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task Three: Many-to-Many Task – Sequence Translation\n",
        "The task is to Translate a sequence of numbers into their squares.\n",
        "\n",
        "Dataset Preparation:"
      ],
      "metadata": {
        "id": "93K5hVNIFL-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset with sequential input and corresponding squared outputs\n",
        "def generate_sequential_many_to_many_data(size, seq_length=5):\n",
        "    X3 = np.zeros((size, seq_length))  # Initialize input array\n",
        "    for i in range(size):\n",
        "        start = np.random.randint(1, 50)  # Random starting point\n",
        "        X3[i] = np.arange(start, start + seq_length)  # Generate sequential numbers\n",
        "    y3 = X3**2  # Output is the square of the input sequence\n",
        "    return X3, y3\n",
        "\n",
        "X3, y3 = generate_sequential_many_to_many_data(1000)\n",
        "\n",
        "# Reshape input for LSTM/GRU\n",
        "X3 = X3.reshape(-1, 5, 1)\n",
        "y3 = y3.reshape(-1, 5, 1)"
      ],
      "metadata": {
        "id": "lzLFA_GHFWBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first few examples"
      ],
      "metadata": {
        "id": "TZprQB-k-SbL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Lk7FQt9tDA",
        "outputId": "95dc3e22-1ba6-4af7-9ae8-0ba30d7011d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example input (X3): [[[22.]\n",
            "  [23.]\n",
            "  [24.]\n",
            "  [25.]\n",
            "  [26.]]\n",
            "\n",
            " [[30.]\n",
            "  [31.]\n",
            "  [32.]\n",
            "  [33.]\n",
            "  [34.]]\n",
            "\n",
            " [[13.]\n",
            "  [14.]\n",
            "  [15.]\n",
            "  [16.]\n",
            "  [17.]]]\n",
            "Example output (y3): [[[ 484.]\n",
            "  [ 529.]\n",
            "  [ 576.]\n",
            "  [ 625.]\n",
            "  [ 676.]]\n",
            "\n",
            " [[ 900.]\n",
            "  [ 961.]\n",
            "  [1024.]\n",
            "  [1089.]\n",
            "  [1156.]]\n",
            "\n",
            " [[ 169.]\n",
            "  [ 196.]\n",
            "  [ 225.]\n",
            "  [ 256.]\n",
            "  [ 289.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Implementation and Training:\n",
        "Implement and train two sequence modeling architectures: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). follow the same steps as you did in task 1\n"
      ],
      "metadata": {
        "id": "sAU8NqnT5dSO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QY3XlD1q5gZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the model\n",
        "Test both of your models using one sequence starting at 2 and predict the output sequence. Verify if the output matches the expected sequence (4,  9, 16, 25, 36)."
      ],
      "metadata": {
        "id": "ezC73xH8-qj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test input\n",
        "X3_test = np.array([[2, 3, 4, 5, 6]])  # A sequence starting at 2\n",
        "X3_test = test_input.reshape(1, 5, 1)  # Reshape to match model input shape"
      ],
      "metadata": {
        "id": "PdSD75Re_Dew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZgqFahO_mGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Improve model performance\n",
        "If your model does not produce the true labels, try to improve it using the techniques you applied in Task 1.\n",
        "\n",
        "Good luck!\n"
      ],
      "metadata": {
        "id": "fVyisqEX_ndJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMjwr4bz_1-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question:**\n",
        "Briefly summarize what you did in this lab. For each task, discuss which model performed better and why. (Note: Write in your own words in Arabic)"
      ],
      "metadata": {
        "id": "jIpBQzXWA915"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3B9sxENH7SC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eHaySflL7c4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmwZfhAN7dnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}